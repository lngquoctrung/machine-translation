{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # üìä Model Evaluation & Testing\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from src.evaluation.metrics import BLEUScore\n",
    "from src.evaluation.beam_search import BeamSearchDecoder\n",
    "from config.model_config import ModelConfig\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Load Models & Tokenizers\n",
    "\n",
    "# %%\n",
    "# Load BiLSTM model\n",
    "bilstm_model = tf.keras.models.load_model(\n",
    "    '../models/saved_models/bilstm_model.h5',\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "# Load LSTM model\n",
    "lstm_model = tf.keras.models.load_model(\n",
    "    '../models/saved_models/lstm_model.h5',\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "# Load tokenizers\n",
    "with open('../models/tokenizers/tokenizer_en.pkl', 'rb') as f:\n",
    "    tokenizer_en = pickle.load(f)\n",
    "\n",
    "with open('../models/tokenizers/tokenizer_vi.pkl', 'rb') as f:\n",
    "    tokenizer_vi = pickle.load(f)\n",
    "\n",
    "print(\"‚úÖ Models and tokenizers loaded\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Setup Decoders\n",
    "\n",
    "# %%\n",
    "config = ModelConfig.to_dict()\n",
    "\n",
    "bilstm_decoder = BeamSearchDecoder(\n",
    "    bilstm_model, tokenizer_en, tokenizer_vi,\n",
    "    config['max_length_en'], config['max_length_vi']\n",
    ")\n",
    "\n",
    "lstm_decoder = BeamSearchDecoder(\n",
    "    lstm_model, tokenizer_en, tokenizer_vi,\n",
    "    config['max_length_en'], config['max_length_vi']\n",
    ")\n",
    "\n",
    "bleu_scorer = BLEUScore()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Test Translations\n",
    "\n",
    "# %%\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love machine learning.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"Can you help me with this problem?\",\n",
    "    \"Thank you for your time.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSLATION TESTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text in test_sentences:\n",
    "    print(f\"\\nüìù Input: {text}\")\n",
    "\n",
    "    # BiLSTM\n",
    "    bilstm_trans = bilstm_decoder.translate(text)\n",
    "    print(f\"üîµ BiLSTM: {bilstm_trans}\")\n",
    "\n",
    "    # LSTM\n",
    "    lstm_trans = lstm_decoder.translate(text)\n",
    "    print(f\"üü¢ LSTM: {lstm_trans}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. BLEU Score Evaluation\n",
    "\n",
    "# %%\n",
    "# Load test data\n",
    "from src.data.preprocessing import DataPreprocessor\n",
    "\n",
    "preprocessor = DataPreprocessor(\n",
    "    max_vocab_en=config['max_vocab_size_en'],\n",
    "    max_vocab_vi=config['max_vocab_size_vi']\n",
    ")\n",
    "\n",
    "df = preprocessor.load_data(\n",
    "    path_en='../data/raw/dataset_en.txt',\n",
    "    path_vi='../data/raw/dataset_vi.txt',\n",
    "    max_length_en=config['max_length_en'],\n",
    "    max_length_vi=config['max_length_vi']\n",
    ")\n",
    "\n",
    "# Get test set\n",
    "_, _, test_df = preprocessor.split_data(df)\n",
    "\n",
    "# %%\n",
    "# Evaluate on test set (sample 100 for speed)\n",
    "test_sample = test_df.sample(n=min(100, len(test_df)))\n",
    "\n",
    "bilstm_bleu_scores = []\n",
    "lstm_bleu_scores = []\n",
    "\n",
    "for idx, row in test_sample.iterrows():\n",
    "    en_text = row['english']\n",
    "    vi_ref = row['vietnamese'].replace('START ', '').replace(' END', '')\n",
    "\n",
    "    # BiLSTM\n",
    "    bilstm_trans = bilstm_decoder.translate(en_text)\n",
    "    bilstm_bleu = bleu_scorer.compute(vi_ref, bilstm_trans)\n",
    "    bilstm_bleu_scores.append(bilstm_bleu)\n",
    "\n",
    "    # LSTM\n",
    "    lstm_trans = lstm_decoder.translate(en_text)\n",
    "    lstm_bleu = bleu_scorer.compute(vi_ref, lstm_trans)\n",
    "    lstm_bleu_scores.append(lstm_bleu)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BLEU SCORE EVALUATION (100 samples)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"BiLSTM - Average BLEU: {np.mean(bilstm_bleu_scores):.2f}\")\n",
    "print(f\"LSTM - Average BLEU: {np.mean(lstm_bleu_scores):.2f}\")\n",
    "print(f\"Improvement: {np.mean(bilstm_bleu_scores) - np.mean(lstm_bleu_scores):.2f} points\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Visualization\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.hist(bilstm_bleu_scores, bins=20, alpha=0.7, label='BiLSTM', edgecolor='black')\n",
    "ax.hist(lstm_bleu_scores, bins=20, alpha=0.7, label='LSTM', edgecolor='black')\n",
    "\n",
    "ax.set_xlabel('BLEU Score')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('BLEU Score Distribution Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../logs/bleu_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Summary Report\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBiLSTM Model:\")\n",
    "print(f\"  - Average BLEU: {np.mean(bilstm_bleu_scores):.2f}\")\n",
    "print(f\"  - Min BLEU: {np.min(bilstm_bleu_scores):.2f}\")\n",
    "print(f\"  - Max BLEU: {np.max(bilstm_bleu_scores):.2f}\")\n",
    "print(f\"  - Parameters: {bilstm_model.count_params():,}\")\n",
    "\n",
    "print(f\"\\nLSTM Model:\")\n",
    "print(f\"  - Average BLEU: {np.mean(lstm_bleu_scores):.2f}\")\n",
    "print(f\"  - Min BLEU: {np.min(lstm_bleu_scores):.2f}\")\n",
    "print(f\"  - Max BLEU: {np.max(lstm_bleu_scores):.2f}\")\n",
    "print(f\"  - Parameters: {lstm_model.count_params():,}\")\n",
    "\n",
    "print(f\"\\nConclusion:\")\n",
    "print(f\"  BiLSTM performs {np.mean(bilstm_bleu_scores) - np.mean(lstm_bleu_scores):.2f} BLEU points better\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
